# Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation

## Author and Organizations：

- Zhehan Kan, Shuoshuo Chen, Ce Zhang, Yushun Tang, Zhihai He
- Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China  Pengcheng Laboratory, Shenzhen, China

## Problems: The generalization problems

The learned network does not have the capability to characterize the prediction error, generate feedback information from the test sample, and correct the prediction error on the fly for each individual test sample, which results in degraded performance in generalization.

## Two main questions:

- how can we tell if the prediction is accurate or not during testing and how to characterize the prediction error?

This is difficult because the ground truth values of the test samples are not available during testing. Specifically, in pose estimation, we do not have the labeled ground truth locations of the body keypoints.

- How to correct the prediction error based on the specific characteristics of the test sample?

Current network models, once successfully trained with labeled samples at the training side, remain fixed during testing, performing the feed forward-only inference process to generate the prediction result.

## Approach:

introduce a ***self-correctable and adaptable inference (SCAI) method*** to address the generalization challenge of network prediction and use human pose estimation as an example to demonstrate its effectiveness and performance. 

We learn a correction network to correct the prediction result conditioned by a fitness feedback error. 

This feedback error is generated by a learned fitness feedback network which maps the prediction result to the original input domain and compares it against the original input. Interestingly, we find that this self-referential feedback error is ***highly correlated*** with the actual prediction error. This strong correlation suggests that we can use this error as feedback to guide the correction process. It can be also used as a loss function to quickly adapt and optimize the correction network during the inference process. 

## Overview:

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled.png)

## Details:

***propose to explore a learning-based feedback-control or correction method for prediction***

- Firstly, let  $\widehat{v} = Φ(u)$  be the prediction network which is tasked to predict the true value of  $v$ from input $u$.
- Secondly, design and learn a fitness feedback network  $Γ$ which compares the prediction result  $\widehat{v} = Φ(u)$ of the prediction network $Φ$ against the original input $u$ and generate a ***self-referential feedback error***. Note that, ***when computing the self-referential error, we do not need the ground truth data.***
- Thirdly, under the guidance of self-referential error feedback, we learn a prediction error correction network  $C$  to produce a correction $∆v = C(\widehat{v}|e_s)$ which adjusts the prediction results   $\widehat{v}$  to   $\tilde{v}=\widehat{v} + ∆v$ , aiming to improve the prediction accuracy on the test samples
- Besides, we find that the self-referential error and the fitness feedback network (FFN) can be used to construct a self-referential loss function on the test samples to quickly adapt and optimize the network model during the inference stage, ***making the model learnable on the test side.***

## How  generate a useful feedback signal $e_s$ for the correction network $C$:

Introduce a fitness feedback network $Γ$. It aims to evaluate how good the corrected prediction result is. 

Note that we ***do not know the ground truth of the prediction result.*** Our idea is to ***map the prediction result to the original input domain*** and compare it against the original input which has its ground truth value. 

Specifically, the fitness feedback network  $Γ$ takes two inputs, the corrected prediction  $\tilde{v}$  and the original input $u$. The output of $Γ$ is the so-called self-referential feedback error $e_s$: $e_s = Γ( \tilde{v}, u)$.  which is used to guide the correction network $C$.

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%201.png)

With successful training, $e_s$ is highly correlated with the prediction error. This strong correlation allows us to use es as feedback to guide the correction of the prediction result. Otherwise, if the correction is weak, the correction process becomes unreliable and cannot achieve improved prediction performance.

When computing this error, we do not need the ground truth value of the prediction. We only need the prediction network $Φ$, the fitness feedback network $Γ$, the correction network $C$, and the input $u$. This implies that we can also compute this feedback error es during the network inference process. Once computed, its norm can be used as a loss function to quickly adapt and optimize the correction network $C$  model during the inference stage using gradient back-propagation. ***During the update, the prediction network $Φ$ and the FFN   $Γ$ remain fixed.*** 

## SCAI for Human Pose Estimation:

### how  re-structure the pose estimation problem to apply the prediction-feedback-correction scheme:

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%202.png)

1. ***Structural groups of body keypoints.*** 

Partition the body keypoints into six structural groups, as depicted in $Figure 2(a)$.

The group is further divided into a distal keypoint $X_D$ at the tip location of the body part, such as the wrists and ankles, and proximal keypoints $X_A, X_B, X_C$ . ***The distal keypoints often have larger errors in prediction due to more significant freedom of motion and possible occlusion by other objects,which is the main objective.***

1. ***SCAI Network Design.***

Given a set of keypoint heatmaps $\{H_A, H_B, H_C , H_D\}$ estimated by a baseline model, our task is to refine heatmaps $\{H_B, H_C , H_D\}$. 

Let us consider the refinement of heatmap $H_D$ of the distal keypoint $X_D$ as an example. 

The input to the prediction network  $\Phi$  is $u = \{H_A, H_B, H_C \}$, the heatmaps for the proximal keypoints. 

The prediction output is $\widehat{v}= \widehat{H_D}$. The correction network $C$ is used to refine the prediction result into $\tilde{v}= \tilde{H_D}$. The output of FFN  $Γ$ is the feedback error $e_s$ to guide the correction.

the prediction network $Φ$ predicts the distal keypoint heatmap  $\tilde{HD} = Φ(H_A, H_B, H_C )$  using heatmaps of proximal keypoints . The correction network $C$ aims to generate a corrected and improved estimation of $H_D$  conditioned by the feedback error $e_s$ 

$$
\tilde{H_D} = \widehat{H_D} + C( \widehat{H_D}|e_s)
$$

Here, $e_s$ is generated by the fitness feedback network $Γ$, which has two inputs, the corrected prediction result $\tilde{H_D}$ and the original inputs

$[H_A, H_B, H_C ]$: 

$$
  e_s = Γ([H_A, H_B, H_C ], \tilde{H_D})
$$

1. ***SCAI Network Training.***

At the training side, all the predicted keypoints have their ground truth values $\{H ^∗ _A, H^∗ _B, H^∗ _C, H^∗ _D\}$. 

Therefore, for the prediction network $Φ$, its loss function is given by $L_Φ = ∥ \widehat{H_D} − H^∗ _D∥^2$.

For the fitness feedback network $Γ$, it maps the prediction result  $\tilde{H_D}$ to the original input domain $\{H_A, H_B, H_C \}$ and compare it against the original input. So, during the training stage, its loss function can be expressed in the form of self-referential error, given by 

$L_Γ = ∥ \widehat{H_A} − H^∗ _A∥_2$. 

For the correction network $C$, its loss function is given by

$$
 L_C = a · L^0 _C + b · L^1 _C + λ · (L^1 _C − L^2 _C )
$$

 where $L^0 _C = ∥ \tilde{H_D} − H^∗ _D∥_2, L^1 _C = ∥\widehat{H_A} − H^∗ _A∥_2$ , and $L^2 _C = ∥ \bar{H_A} − H^∗ _A∥_2$ with $a$, $b$ and $λ$ representing the weights for the three losses, respectively. 

$L^1 _C$ is defined by the distance between the mapping from the corrected prediction$\tilde{H_D}$ and the original input.

 $L^2 _C$ is defined by the distance between the mapping from the uncorrected prediction $\widehat{H_D}$   and the original input.

Here, the last term $L^1_C − L^2_C$ is used to ensure the effectiveness of correction: **the corrected result has a smaller self-referential error than the original prediction.** 

During the training process, the prediction network $Φ$ is pre-trained using the training samples $\{[(H_A, H_B, H_C ) → H_D]\}$. 

The FFN is also pre-trained with training samples $\{[(H_B, H_C , H_D) → H_A]\}.$ Here, $→$ represents the network prediction.

During the training stage, the prediction network $Φ$ is fixed. The pre-trained model of the FFN $Γ$ is used as its initial model. The FFN and the correction networks are then jointly trained using their loss functions. 

1. ***Self-referential Adaptable Inference for Human Pose Estimation.***

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%203.png)

***On the test side, we use this self-referential error as a loss function to update the network model.*** 

Specifically,  we choose to update the correction network   $C$  while other networks, including the prediction network $Φ$ and the FNN $Γ$ remain fixed. It should be noted that this model refinement is performed separately for each batch of test images. When moving to a new batch, the initial models obtained from the training set are restored and then refined. In other words, the models learned from one batch are not used for the next test batch to ensure flexible model adaptation.

$Figure 4(a)$ shows the decreasing self-referential error and the convergence behavior of this model learning and adapta-tion process. $Figure 4(b)$ shows that the accuracy of the test batch is consistently improved with the training epochs. 

***self-referential adaptable inference method is able to use the test samples as feedback to update the network models and improve the prediction performance, providing natural and enhanced generalization capability for the learning and prediction network.***

## Results:

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%204.png)

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%205.png)

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%206.png)

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%207.png)

## Ablation Studies:

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%208.png)

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%209.png)

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%2010.png)

![Untitled](Self-Correctable%20and%20Adaptable%20Inference%20for%20Gener%203a007163c0494828868c1640130fe5d1/Untitled%2011.png)

## Further Discussions and Summary of Unique Contributions:

(1) Unique Differences from Related Work.

(2) Algorithm complexity.